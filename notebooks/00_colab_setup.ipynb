{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1468b1cf",
   "metadata": {},
   "source": [
    "> **This notebook sets up our entire project environment in Google Colab with GPU support**\n",
    "## Before Running This Notebook\n",
    "\n",
    "1. **Enable GPU**: Go to `Runtime` â†’ `Change runtime type` â†’ `Hardware accelerator` â†’ `T4 GPU`\n",
    "2. **Connect Drive** (optional): To save large model files between sessions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3af7ae",
   "metadata": {},
   "source": [
    "## Step 1: Downloading GitHub repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef17939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Remove existing directory if it exists (for re-running)\n",
    "if os.path.exists('LLM-interface-optimization'):\n",
    "    !rm -rf LLM-interface-optimization\n",
    "\n",
    "# Clone the repo\n",
    "!git clone https://github.com/Bekmukhamed/LLM-interface-optimization.git\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('LLM-interface-optimization')\n",
    "\n",
    "# List files to verify download\n",
    "print(\"Project files downloaded:\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153dbe1",
   "metadata": {},
   "source": [
    "## Step 2: Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing specific requirements\n",
    "print(\"Installing project dependencies...\")\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Installing additional Colab-specific packages\n",
    "print(\"\\nInstalling Colab-specific optimizations...\")\n",
    "!pip install accelerate>=0.24.0  # GPU acceleration\n",
    "!pip install xformers>=0.0.20    # Memory-efficient transformers (when available)\n",
    "\n",
    "print(\"\\nPackage installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49d2da8",
   "metadata": {},
   "source": [
    "## Step 3: Verifying GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"GPU SETUP VERIFICATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    current_gpu = torch.cuda.current_device()\n",
    "    gpu_name = torch.cuda.get_device_name(current_gpu)\n",
    "    gpu_memory = torch.cuda.get_device_properties(current_gpu).total_memory / 1e9\n",
    "    \n",
    "    print(f\"GPU Status: AVAILABLE\")\n",
    "    print(f\"GPU Name: {gpu_name}\")\n",
    "    print(f\"GPU Count: {gpu_count}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Test GPU computation\n",
    "    test_tensor = torch.randn(1000, 1000).cuda()\n",
    "    result = torch.matmul(test_tensor, test_tensor)\n",
    "    print(f\"GPU computation test: SUCCESS\")\n",
    "    \n",
    "else:\n",
    "    print(\"GPU Status: NOT AVAILABLE\")\n",
    "    print(\"Hot to fix: Go to Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\")\n",
    "\n",
    "# Show detailed GPU info\n",
    "print(\"\\nðŸ“‹ Detailed GPU Information:\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395dd8d1",
   "metadata": {},
   "source": [
    "## Step 4: Run Environment Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d01575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our environment verification script\n",
    "print(\"Running comprehensive environment tests...\")\n",
    "print()\n",
    "\n",
    "!python test_environment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0abf5f6",
   "metadata": {},
   "source": [
    "## Step 5: Load Your First LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2879f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "\n",
    "print(\"LOADING FIRST LLM MODEL\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Small fast model\n",
    "model_name = \"distilgpt2\"  # Small version of GPT-2 (82M parameters)\n",
    "print(f\"Downloading model: {model_name}\")\n",
    "print(\"(This may take 1-2 minutes on first run)\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Running on: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Add padding token (required for some operations)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80767ae",
   "metadata": {},
   "source": [
    "## Step 6: First Text Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=50):\n",
    "    \"\"\"Generating text and measuring performance\"\"\"\n",
    "    print(f\"Input prompt: '{prompt}'\")\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Measure generation time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    generation_time = end_time - start_time\n",
    "    tokens_generated = len(outputs[0]) - len(inputs.input_ids[0])\n",
    "    tokens_per_second = tokens_generated / generation_time\n",
    "    \n",
    "    print(f\"Generated text: '{generated_text}'\")\n",
    "    print(f\"Generation time: {generation_time:.3f} seconds\")\n",
    "    print(f\"Tokens generated: {tokens_generated}\")\n",
    "    print(f\"Speed: {tokens_per_second:.1f} tokens/second\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        'text': generated_text,\n",
    "        'time': generation_time,\n",
    "        'tokens': tokens_generated,\n",
    "        'speed': tokens_per_second\n",
    "    }\n",
    "\n",
    "# Test with different prompts\n",
    "print(\"BASELINE PERFORMANCE TESTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_prompts = [\n",
    "    \"Hello, \",\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"The best way to optimize neural networks\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nTest {i}/3:\")\n",
    "    result = generate_text(prompt)\n",
    "    results.append(result)\n",
    "\n",
    "# Summary statistics\n",
    "avg_speed = sum(r['speed'] for r in results) / len(results)\n",
    "avg_time = sum(r['time'] for r in results) / len(results)\n",
    "\n",
    "print(\"\\nBASELINE PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Average speed: {avg_speed:.1f} tokens/second\")\n",
    "print(f\"Average time: {avg_time:.3f} seconds\")\n",
    "print(f\"Device used: {device}\")\n",
    "print()\n",
    "print(\"NEXT STEPS: Now we'll work on optimizing this speed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557612a",
   "metadata": {},
   "source": [
    "## Step 7: Save Environment State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef719d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create environment report\n",
    "env_report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'platform': 'Google Colab',\n",
    "    'device': str(device),\n",
    "    'cuda_available': torch.cuda.is_available(),\n",
    "    'torch_version': torch.__version__,\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    env_report.update({\n",
    "        'gpu_name': torch.cuda.get_device_name(0),\n",
    "        'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "        'cuda_version': torch.version.cuda\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open('colab_environment.json', 'w') as f:\n",
    "    json.dump(env_report, f, indent=2)\n",
    "\n",
    "print(\"Environment state saved to 'colab_environment.json'\")\n",
    "print(\"Environment summary:\")\n",
    "for key, value in env_report.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ffc1d",
   "metadata": {},
   "source": [
    "## Setup Complete\n",
    "\n",
    "Google Colab environment is now ready for LLM optimization experiments.\n",
    "\n",
    "- Downloaded project from GitHub\n",
    "- Installed all dependencies\n",
    "- Verified GPU access\n",
    "- Loaded your first LLM model\n",
    "- Measured baseline performance\n",
    "\n",
    "### Notebooks\n",
    "- `02_model_optimization.ipynb` - TensorRT optimization\n",
    "- `03_custom_kernels.ipynb` - CUDA kernel development\n",
    "- `04_benchmarking.ipynb` - Performance measurement\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
