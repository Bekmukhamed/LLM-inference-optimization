{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e0fea5",
   "metadata": {},
   "source": [
    "# LLM Performance Benchmarking\n",
    "\n",
    "> **Measure baseline performance before optimization - the foundation of any optimization project**\n",
    "\n",
    "1. **Loads Multiple Models**: Test different sizes (small to large)\n",
    "2. **Measures Key Metrics**: Latency, throughput, memory usage\n",
    "3. **Identifies Bottlenecks**: CPU vs GPU vs Memory limitations\n",
    "4. **Creates Baseline Report**: Professional performance analysis\n",
    "5. **Guides Optimization**: Shows exactly what to optimize next\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498e514",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d27e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab authentication setup\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import userdata\n",
    "    import os\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Local environment - no token setup needed\")\n",
    "\n",
    "# Essential imports for benchmarking\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Benchmarking tools loaded successfully!\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376111c",
   "metadata": {},
   "source": [
    "## Model Selection for Benchmarking\n",
    "\n",
    "**Strategy**: Test multiple model sizes to understand scaling behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48815d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to benchmark (small to large)\n",
    "BENCHMARK_MODELS = [\n",
    "    {\n",
    "        'name': 'DistilGPT-2',\n",
    "        'model_id': 'distilgpt2',\n",
    "        'params': '82M',\n",
    "        'description': 'Smallest - fast baseline'\n",
    "    },\n",
    "    {\n",
    "        'name': 'GPT-2 Small',\n",
    "        'model_id': 'gpt2',\n",
    "        'params': '124M', \n",
    "        'description': 'Small - good for development'\n",
    "    },\n",
    "    {\n",
    "        'name': 'GPT-2 Medium',\n",
    "        'model_id': 'gpt2-medium',\n",
    "        'params': '355M',\n",
    "        'description': 'Medium - realistic workload'\n",
    "    }\n",
    "    # Note: We can add larger models later if needed\n",
    "    # GPT-2 Large (774M) and XL (1.5B) - for advanced testing\n",
    "]\n",
    "\n",
    "# Test prompts of different lengths\n",
    "TEST_PROMPTS = [\n",
    "    \"AI optimization is\",  # Short prompt\n",
    "    \"The future of artificial intelligence and machine learning technologies will\",  # Medium prompt\n",
    "    \"In the rapidly evolving field of artificial intelligence, researchers are constantly developing new optimization techniques that\"  # Long prompt\n",
    "]\n",
    "\n",
    "# Benchmark configuration\n",
    "BENCHMARK_CONFIG = {\n",
    "    'max_new_tokens': [10, 50, 100],  # Different generation lengths\n",
    "    'batch_sizes': [1, 4],             # Single vs batch inference\n",
    "    'num_runs': 5,                     # Repetitions for statistical reliability\n",
    "    'warmup_runs': 2                   # Ignore first runs (GPU warmup)\n",
    "}\n",
    "\n",
    "print(\"Benchmark Configuration:\")\n",
    "print(f\"   Models to test: {len(BENCHMARK_MODELS)}\")\n",
    "print(f\"   Test prompts: {len(TEST_PROMPTS)}\")\n",
    "print(f\"   Generation lengths: {BENCHMARK_CONFIG['max_new_tokens']}\")\n",
    "print(f\"   Batch sizes: {BENCHMARK_CONFIG['batch_sizes']}\")\n",
    "print(f\"   Runs per test: {BENCHMARK_CONFIG['num_runs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b2205",
   "metadata": {},
   "source": [
    "## Benchmarking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42749501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory_usage():\n",
    "    \"\"\"Get current memory usage (CPU and GPU)\"\"\"\n",
    "    memory_info = {\n",
    "        'cpu_percent': psutil.virtual_memory().percent,\n",
    "        'cpu_used_gb': psutil.virtual_memory().used / 1e9\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        memory_info.update({\n",
    "            'gpu_allocated_gb': torch.cuda.memory_allocated() / 1e9,\n",
    "            'gpu_reserved_gb': torch.cuda.memory_reserved() / 1e9,\n",
    "            'gpu_total_gb': torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        })\n",
    "        memory_info['gpu_utilization_percent'] = (\n",
    "            memory_info['gpu_allocated_gb'] / memory_info['gpu_total_gb'] * 100\n",
    "        )\n",
    "    \n",
    "    return memory_info\n",
    "\n",
    "def benchmark_model_loading(model_id):\n",
    "    \"\"\"Measure model loading time and memory impact\"\"\"\n",
    "    print(f\"Loading {model_id}...\")\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Measure before loading\n",
    "    memory_before = measure_memory_usage()\n",
    "    \n",
    "    # Load model and measure time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    loading_time = time.time() - start_time\n",
    "    \n",
    "    # Measure after loading\n",
    "    memory_after = measure_memory_usage()\n",
    "    \n",
    "    # Add padding token if needed\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Calculate model size\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / 1e6\n",
    "    \n",
    "    loading_stats = {\n",
    "        'loading_time_sec': loading_time,\n",
    "        'param_count': param_count,\n",
    "        'model_size_mb': model_size_mb,\n",
    "        'memory_before': memory_before,\n",
    "        'memory_after': memory_after,\n",
    "        'device': str(device)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Loaded in {loading_time:.2f}s, {param_count:,} parameters, {model_size_mb:.1f}MB\")\n",
    "    \n",
    "    return model, tokenizer, loading_stats\n",
    "\n",
    "def benchmark_inference(model, tokenizer, prompt, max_new_tokens=50, batch_size=1, num_runs=5):\n",
    "    \"\"\"Measure inference performance with statistical reliability\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Prepare inputs\n",
    "    if batch_size == 1:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    else:\n",
    "        # Create batch by repeating prompt\n",
    "        prompts = [prompt] * batch_size\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    # Warmup runs (GPU needs to \"warm up\")\n",
    "    for _ in range(2):\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,  # Deterministic for consistent timing\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "    \n",
    "    # Actual benchmark runs\n",
    "    latencies = []\n",
    "    memory_stats = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        # Clear cache and measure initial memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()  # Ensure GPU operations complete\n",
    "        \n",
    "        memory_before = measure_memory_usage()\n",
    "        \n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()  # Ensure GPU completes\n",
    "        \n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        \n",
    "        memory_after = measure_memory_usage()\n",
    "        \n",
    "        latencies.append(latency)\n",
    "        memory_stats.append({\n",
    "            'before': memory_before,\n",
    "            'after': memory_after\n",
    "        })\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_latency = np.mean(latencies)\n",
    "    std_latency = np.std(latencies)\n",
    "    min_latency = np.min(latencies)\n",
    "    max_latency = np.max(latencies)\n",
    "    \n",
    "    # Calculate tokens and throughput\n",
    "    total_tokens_generated = (len(outputs[0]) - len(inputs.input_ids[0])) * batch_size\n",
    "    tokens_per_second = total_tokens_generated / mean_latency\n",
    "    \n",
    "    # Decode first output for verification\n",
    "    sample_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    inference_stats = {\n",
    "        'prompt': prompt,\n",
    "        'max_new_tokens': max_new_tokens,\n",
    "        'batch_size': batch_size,\n",
    "        'mean_latency_sec': mean_latency,\n",
    "        'std_latency_sec': std_latency,\n",
    "        'min_latency_sec': min_latency,\n",
    "        'max_latency_sec': max_latency,\n",
    "        'tokens_generated': total_tokens_generated,\n",
    "        'tokens_per_second': tokens_per_second,\n",
    "        'sample_output': sample_output,\n",
    "        'memory_stats': memory_stats\n",
    "    }\n",
    "    \n",
    "    return inference_stats\n",
    "\n",
    "print(\"Benchmarking functions ready!\")\n",
    "print(\"Can measure: loading time, inference latency, memory usage, throughput\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1af54",
   "metadata": {},
   "source": [
    "## Run Comprehensive Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4478f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all benchmark results\n",
    "all_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'environment': {\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
    "        'torch_version': torch.__version__,\n",
    "        'cuda_version': torch.version.cuda if torch.cuda.is_available() else None\n",
    "    },\n",
    "    'models': []\n",
    "}\n",
    "\n",
    "print(\"Starting Comprehensive Benchmark Suite\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Benchmark each model\n",
    "for model_config in BENCHMARK_MODELS:\n",
    "    print(f\"\n",
    "Testing {model_config['name']} ({model_config['params']})\")\n",
    "    print(f\"   Description: {model_config['description']}\")\n",
    "    \n",
    "    try:\n",
    "        # Load model and measure loading performance\n",
    "        model, tokenizer, loading_stats = benchmark_model_loading(model_config['model_id'])\n",
    "        \n",
    "        model_results = {\n",
    "            'config': model_config,\n",
    "            'loading_stats': loading_stats,\n",
    "            'inference_results': []\n",
    "        }\n",
    "        \n",
    "        # Test different scenarios\n",
    "        print(\"   Running inference benchmarks...\")\n",
    "        \n",
    "        # Test first prompt with different generation lengths\n",
    "        test_prompt = TEST_PROMPTS[0]  # Start with short prompt\n",
    "        \n",
    "        for max_tokens in BENCHMARK_CONFIG['max_new_tokens']:\n",
    "            for batch_size in BENCHMARK_CONFIG['batch_sizes']:\n",
    "                print(f\"      {max_tokens} tokens, batch_size={batch_size}...\")\n",
    "                \n",
    "                result = benchmark_inference(\n",
    "                    model, tokenizer, test_prompt,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    batch_size=batch_size,\n",
    "                    num_runs=BENCHMARK_CONFIG['num_runs']\n",
    "                )\n",
    "                \n",
    "                model_results['inference_results'].append(result)\n",
    "                \n",
    "                print(f\"         Latency: {result['mean_latency_sec']:.3f}±{result['std_latency_sec']:.3f}s\")\n",
    "                print(f\"         Throughput: {result['tokens_per_second']:.1f} tokens/sec\")\n",
    "        \n",
    "        all_results['models'].append(model_results)\n",
    "        \n",
    "        # Clean up to free memory for next model\n",
    "        del model, tokenizer\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"   {model_config['name']} benchmarking complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error benchmarking {model_config['name']}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\n",
    "Benchmark suite completed!\")\n",
    "print(f\"Tested {len(all_results['models'])} models successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa2811",
   "metadata": {},
   "source": [
    "## Performance Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1da963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for visualization\n",
    "model_names = []\n",
    "param_counts = []\n",
    "loading_times = []\n",
    "latencies_50_tokens = []  # 50 token generation, batch_size=1\n",
    "throughputs_50_tokens = []\n",
    "memory_usage = []\n",
    "\n",
    "for model_result in all_results['models']:\n",
    "    model_names.append(model_result['config']['name'])\n",
    "    param_counts.append(model_result['loading_stats']['param_count'] / 1e6)  # Convert to millions\n",
    "    loading_times.append(model_result['loading_stats']['loading_time_sec'])\n",
    "    \n",
    "    # Find 50 token, batch_size=1 result\n",
    "    for inference_result in model_result['inference_results']:\n",
    "        if inference_result['max_new_tokens'] == 50 and inference_result['batch_size'] == 1:\n",
    "            latencies_50_tokens.append(inference_result['mean_latency_sec'])\n",
    "            throughputs_50_tokens.append(inference_result['tokens_per_second'])\n",
    "            \n",
    "            # Memory usage (GPU if available)\n",
    "            if torch.cuda.is_available():\n",
    "                memory_usage.append(model_result['loading_stats']['memory_after']['gpu_allocated_gb'])\n",
    "            else:\n",
    "                memory_usage.append(model_result['loading_stats']['memory_after']['cpu_used_gb'])\n",
    "            break\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('LLM Performance Baseline Report', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Model Size vs Loading Time\n",
    "ax1.scatter(param_counts, loading_times, s=100, alpha=0.7, color='blue')\n",
    "ax1.set_xlabel('Model Size (Million Parameters)')\n",
    "ax1.set_ylabel('Loading Time (seconds)')\n",
    "ax1.set_title('Model Loading Performance')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "for i, name in enumerate(model_names):\n",
    "    ax1.annotate(name, (param_counts[i], loading_times[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# 2. Model Size vs Inference Latency\n",
    "ax2.scatter(param_counts, latencies_50_tokens, s=100, alpha=0.7, color='red')\n",
    "ax2.set_xlabel('Model Size (Million Parameters)')\n",
    "ax2.set_ylabel('Inference Latency (seconds)')\n",
    "ax2.set_title('Inference Performance (50 tokens)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "for i, name in enumerate(model_names):\n",
    "    ax2.annotate(name, (param_counts[i], latencies_50_tokens[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# 3. Throughput Comparison\n",
    "bars = ax3.bar(model_names, throughputs_50_tokens, color='green', alpha=0.7)\n",
    "ax3.set_ylabel('Throughput (tokens/second)')\n",
    "ax3.set_title('Generation Throughput')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "# Add value labels on bars\n",
    "for i, (bar, value) in enumerate(zip(bars, throughputs_50_tokens)):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{value:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 4. Memory Usage\n",
    "bars = ax4.bar(model_names, memory_usage, color='purple', alpha=0.7)\n",
    "memory_type = 'GPU' if torch.cuda.is_available() else 'CPU'\n",
    "ax4.set_ylabel(f'{memory_type} Memory Usage (GB)')\n",
    "ax4.set_title(f'{memory_type} Memory Consumption')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "# Add value labels on bars\n",
    "for i, (bar, value) in enumerate(zip(bars, memory_usage)):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "             f'{value:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\n",
    "PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "for i, model_result in enumerate(all_results['models']):\n",
    "    config = model_result['config']\n",
    "    loading = model_result['loading_stats']\n",
    "    \n",
    "    print(f\"\n",
    "{config['name']} ({config['params']})\")\n",
    "    print(f\"   Loading: {loading['loading_time_sec']:.2f}s\")\n",
    "    print(f\"   Memory: {memory_usage[i]:.2f} GB\")\n",
    "    print(f\"   Latency: {latencies_50_tokens[i]:.3f}s (50 tokens)\")\n",
    "    print(f\"   Throughput: {throughputs_50_tokens[i]:.1f} tokens/sec\")\n",
    "\n",
    "# Identify optimization opportunities\n",
    "print(\"\n",
    "OPTIMIZATION OPPORTUNITIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find slowest model for optimization focus\n",
    "slowest_idx = latencies_50_tokens.index(max(latencies_50_tokens))\n",
    "fastest_idx = latencies_50_tokens.index(min(latencies_50_tokens))\n",
    "\n",
    "print(f\"Slowest model: {model_names[slowest_idx]} ({latencies_50_tokens[slowest_idx]:.3f}s)\")\n",
    "print(f\"Fastest model: {model_names[fastest_idx]} ({latencies_50_tokens[fastest_idx]:.3f}s)\")\n",
    "print(f\"Speedup potential: {latencies_50_tokens[slowest_idx]/latencies_50_tokens[fastest_idx]:.1f}x\")\n",
    "\n",
    "# Memory analysis\n",
    "max_memory_idx = memory_usage.index(max(memory_usage))\n",
    "print(f\"Most memory hungry: {model_names[max_memory_idx]} ({memory_usage[max_memory_idx]:.2f} GB)\")\n",
    "\n",
    "# Scaling analysis\n",
    "if len(param_counts) > 1:\n",
    "    # Simple linear fit to understand scaling\n",
    "    param_ratio = param_counts[-1] / param_counts[0]\n",
    "    latency_ratio = latencies_50_tokens[-1] / latencies_50_tokens[0]\n",
    "    print(f\"Scaling analysis: {param_ratio:.1f}x parameters → {latency_ratio:.1f}x latency\")\n",
    "    \n",
    "    if latency_ratio > param_ratio:\n",
    "        print(\"Worse than linear scaling - optimization needed!\")\n",
    "    else:\n",
    "        print(\"Better than linear scaling - good baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63a9ee",
   "metadata": {},
   "source": [
    "## Save Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e5ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to JSON\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"baseline_benchmark_{timestamp}.json\"\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Detailed results saved to: {filename}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = {\n",
    "    'benchmark_date': datetime.now().isoformat(),\n",
    "    'environment': all_results['environment'],\n",
    "    'models_tested': len(all_results['models']),\n",
    "    'key_findings': {\n",
    "        'fastest_model': {\n",
    "            'name': model_names[fastest_idx],\n",
    "            'latency_sec': latencies_50_tokens[fastest_idx],\n",
    "            'throughput_tokens_per_sec': throughputs_50_tokens[fastest_idx]\n",
    "        },\n",
    "        'slowest_model': {\n",
    "            'name': model_names[slowest_idx],\n",
    "            'latency_sec': latencies_50_tokens[slowest_idx],\n",
    "            'throughput_tokens_per_sec': throughputs_50_tokens[slowest_idx]\n",
    "        },\n",
    "        'optimization_potential': {\n",
    "            'max_speedup_possible': round(latencies_50_tokens[slowest_idx]/latencies_50_tokens[fastest_idx], 1),\n",
    "            'memory_range_gb': [min(memory_usage), max(memory_usage)]\n",
    "        }\n",
    "    },\n",
    "    'next_optimization_targets': [\n",
    "        'TensorRT optimization for fastest inference',\n",
    "        'Quantization for memory reduction',\n",
    "        'Custom CUDA kernels for specialized operations',\n",
    "        'Batching optimization for throughput'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_filename = f\"benchmark_summary_{timestamp}.json\"\n",
    "with open(summary_filename, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(f\"Summary report saved to: {summary_filename}\")\n",
    "\n",
    "# Display final summary\n",
    "print(\"\n",
    "BENCHMARKING COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"What you now have:\")\n",
    "print(\"   Baseline performance measurements\")\n",
    "print(\"   Performance scaling analysis\")\n",
    "print(\"   Clear optimization targets\")\n",
    "print(\"   Professional benchmark reports\")\n",
    "print(\"\n",
    "Next steps: Choose optimization technique to implement!\")\n",
    "print(\"   1. TensorRT optimization (fastest path to speedup)\")\n",
    "print(\"   2. Quantization (memory reduction)\")\n",
    "print(\"   3. Custom CUDA kernels (maximum performance)\")\n",
    "print(\"   4. Batching optimization (throughput improvement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb6829",
   "metadata": {},
   "source": [
    "## Benchmark Results Analysis\n",
    "\n",
    "### Next Optimization Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b508b2e",
   "metadata": {},
   "source": [
    "## Download Results from Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43249db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results files to your local machine\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"Available result files to download:\")\n",
    "    \n",
    "    # Download benchmark results\n",
    "    results_file = f\"benchmark_results_{timestamp}.json\"\n",
    "    if os.path.exists(results_file):\n",
    "        print(f\"Done{results_file}\")\n",
    "        files.download(results_file)\n",
    "    \n",
    "    # Download summary report\n",
    "    summary_file = f\"benchmark_summary_{timestamp}.json\"\n",
    "    if os.path.exists(summary_file):\n",
    "        print(f\"Done{summary_file}\")\n",
    "        files.download(summary_file)\n",
    "    \n",
    "    # Download any plots that were saved\n",
    "    import glob\n",
    "    plot_files = glob.glob(\"*benchmark*.png\")\n",
    "    for plot_file in plot_files:\n",
    "        print(f\"Done{plot_file}\")\n",
    "        files.download(plot_file)\n",
    "    \n",
    "    print(\"\\nAll files downloaded to your Downloads folder!\")\n",
    "    print(\"Upload these to your GitHub repository for documentation\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Local environment - files already saved locally:\")\n",
    "    print(f\"{results_file}\")\n",
    "    print(f\"{summary_file}\")\n",
    "    print(\"benchmark_plots.png\")\n",
    "    print(\"\\nNext: Commit these to your GitHub repository\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
