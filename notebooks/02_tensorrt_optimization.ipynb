{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c8e46c",
   "metadata": {},
   "source": [
    "# TensorRT Model Optimization\n",
    "\n",
    "> **Convert PyTorch models to optimized TensorRT engines for 2-5x speedup**\n",
    "\n",
    "1. **Convert Models**: PyTorch → TensorRT engines\n",
    "2. **Measure Speedup**: Compare before/after performance  \n",
    "3. **Optimize Settings**: Precision, batch size, sequence length\n",
    "4. **Save Engines**: Reusable optimized models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed4286f",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorRT optimization tools loaded!\")\n",
    "print(f\"TensorRT version: {trt.__version__}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37a411",
   "metadata": {},
   "source": [
    "## TensorRT Engine Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorRTOptimizer:\n",
    "    def __init__(self, precision='fp16'):\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        self.builder = trt.Builder(self.logger)\n",
    "        self.precision = precision\n",
    "        \n",
    "    def build_engine(self, onnx_path, engine_path, max_batch_size=1, max_seq_len=512):\n",
    "        \"\"\"Build TensorRT engine from ONNX model\"\"\"\n",
    "        \n",
    "        # Create network\n",
    "        network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "        network = self.builder.create_network(network_flags)\n",
    "        parser = trt.OnnxParser(network, self.logger)\n",
    "        \n",
    "        # Parse ONNX model\n",
    "        with open(onnx_path, 'rb') as model:\n",
    "            if not parser.parse(model.read()):\n",
    "                print(\"ERROR: Failed to parse ONNX model\")\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                return None\n",
    "        \n",
    "        # Create builder config\n",
    "        config = self.builder.create_builder_config()\n",
    "        \n",
    "        # Set precision\n",
    "        if self.precision == 'fp16' and self.builder.platform_has_fast_fp16:\n",
    "            config.set_flag(trt.BuilderFlag.FP16)\n",
    "            print(\"Using FP16 precision\")\n",
    "        elif self.precision == 'int8' and self.builder.platform_has_fast_int8:\n",
    "            config.set_flag(trt.BuilderFlag.INT8)\n",
    "            print(\"Using INT8 precision\")\n",
    "        else:\n",
    "            print(\"Using FP32 precision\")\n",
    "        \n",
    "        # Set memory pool\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 2 << 30)  # 2GB\n",
    "        \n",
    "        # Build engine\n",
    "        print(\"Building TensorRT engine... (this may take several minutes)\")\n",
    "        engine = self.builder.build_serialized_network(network, config)\n",
    "        \n",
    "        if engine is None:\n",
    "            print(\"ERROR: Failed to build engine\")\n",
    "            return None\n",
    "        \n",
    "        # Save engine\n",
    "        with open(engine_path, 'wb') as f:\n",
    "            f.write(engine)\n",
    "        \n",
    "        print(f\"Engine saved to {engine_path}\")\n",
    "        return engine\n",
    "\n",
    "# Initialize optimizer\n",
    "trt_optimizer = TensorRTOptimizer(precision='fp16')\n",
    "print(\"TensorRT optimizer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e278a7e",
   "metadata": {},
   "source": [
    "## Model to ONNX Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84399962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_onnx(model_name, onnx_path, max_seq_len=128):\n",
    "    \"\"\"Convert PyTorch model to ONNX format\"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randint(0, tokenizer.vocab_size, (1, max_seq_len))\n",
    "    \n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input_ids'],\n",
    "        output_names=['logits'],\n",
    "        dynamic_axes={\n",
    "            'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "            'logits': {0: 'batch_size', 1: 'sequence'}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Model exported to {onnx_path}\")\n",
    "    return tokenizer\n",
    "\n",
    "# Convert models to ONNX\n",
    "models_to_optimize = ['distilgpt2', 'gpt2']\n",
    "onnx_models = {}\n",
    "\n",
    "for model_name in models_to_optimize:\n",
    "    onnx_path = f\"{model_name.replace('/', '_')}.onnx\"\n",
    "    print(f\"Converting {model_name} to ONNX...\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = convert_to_onnx(model_name, onnx_path, max_seq_len=128)\n",
    "        onnx_models[model_name] = {\n",
    "            'onnx_path': onnx_path,\n",
    "            'tokenizer': tokenizer\n",
    "        }\n",
    "        print(f\"✓ {model_name} converted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to convert {model_name}: {e}\")\n",
    "\n",
    "print(f\"Converted {len(onnx_models)} models to ONNX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd83de",
   "metadata": {},
   "source": [
    "## Build TensorRT Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TensorRT engines from ONNX models\n",
    "trt_engines = {}\n",
    "\n",
    "for model_name, model_info in onnx_models.items():\n",
    "    engine_path = f\"{model_name.replace('/', '_')}_fp16.trt\"\n",
    "    print(f\"Building TensorRT engine for {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        engine = trt_optimizer.build_engine(\n",
    "            onnx_path=model_info['onnx_path'],\n",
    "            engine_path=engine_path,\n",
    "            max_batch_size=4,\n",
    "            max_seq_len=128\n",
    "        )\n",
    "        \n",
    "        if engine is not None:\n",
    "            trt_engines[model_name] = {\n",
    "                'engine_path': engine_path,\n",
    "                'tokenizer': model_info['tokenizer']\n",
    "            }\n",
    "            print(f\"✓ {model_name} engine built successfully\")\n",
    "        else:\n",
    "            print(f\"✗ Failed to build engine for {model_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error building engine for {model_name}: {e}\")\n",
    "\n",
    "print(f\"Built {len(trt_engines)} TensorRT engines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428ef22",
   "metadata": {},
   "source": [
    "## TensorRT Inference Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c128a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorRTInference:\n",
    "    def __init__(self, engine_path):\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        \n",
    "        # Load engine\n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_data = f.read()\n",
    "        \n",
    "        runtime = trt.Runtime(self.logger)\n",
    "        self.engine = runtime.deserialize_cuda_engine(engine_data)\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Allocate GPU memory\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.bindings = []\n",
    "        \n",
    "        for binding in self.engine:\n",
    "            binding_idx = self.engine.get_binding_index(binding)\n",
    "            size = trt.volume(self.context.get_binding_shape(binding_idx))\n",
    "            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n",
    "            \n",
    "            # Allocate host and device buffers\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            \n",
    "            self.bindings.append(int(device_mem))\n",
    "            \n",
    "            if self.engine.binding_is_input(binding):\n",
    "                self.inputs.append({'host': host_mem, 'device': device_mem})\n",
    "            else:\n",
    "                self.outputs.append({'host': host_mem, 'device': device_mem})\n",
    "    \n",
    "    def infer(self, input_data):\n",
    "        \"\"\"Run inference with TensorRT engine\"\"\"\n",
    "        \n",
    "        # Copy input data to GPU\n",
    "        np.copyto(self.inputs[0]['host'], input_data.ravel())\n",
    "        cuda.memcpy_htod(self.inputs[0]['device'], self.inputs[0]['host'])\n",
    "        \n",
    "        # Run inference\n",
    "        self.context.execute_v2(bindings=self.bindings)\n",
    "        \n",
    "        # Copy output data from GPU\n",
    "        cuda.memcpy_dtoh(self.outputs[0]['host'], self.outputs[0]['device'])\n",
    "        \n",
    "        return self.outputs[0]['host']\n",
    "\n",
    "# Note: This requires pycuda which may not be available in Colab\n",
    "# Alternative: Use torch-tensorrt or other high-level interfaces\n",
    "print(\"TensorRT inference class defined\")\n",
    "print(\"Note: Full TensorRT inference requires additional CUDA setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4068e",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d886485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pytorch_model(model_name, num_runs=10):\n",
    "    \"\"\"Benchmark original PyTorch model\"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Test prompt\n",
    "    prompt = \"The future of artificial intelligence\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(inputs.input_ids, max_new_tokens=50, do_sample=False)\n",
    "    \n",
    "    # Benchmark\n",
    "    latencies = []\n",
    "    for _ in range(num_runs):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(inputs.input_ids, max_new_tokens=50, do_sample=False)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        latencies.append(latency)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_latency = np.mean(latencies)\n",
    "    std_latency = np.std(latencies)\n",
    "    \n",
    "    # Calculate throughput\n",
    "    tokens_generated = len(outputs[0]) - len(inputs.input_ids[0])\n",
    "    throughput = tokens_generated / mean_latency\n",
    "    \n",
    "    return {\n",
    "        'mean_latency': mean_latency,\n",
    "        'std_latency': std_latency,\n",
    "        'throughput': throughput,\n",
    "        'tokens_generated': tokens_generated\n",
    "    }\n",
    "\n",
    "# Benchmark PyTorch models\n",
    "pytorch_results = {}\n",
    "\n",
    "for model_name in models_to_optimize:\n",
    "    print(f\"Benchmarking PyTorch {model_name}...\")\n",
    "    try:\n",
    "        result = benchmark_pytorch_model(model_name)\n",
    "        pytorch_results[model_name] = result\n",
    "        print(f\"  Latency: {result['mean_latency']:.3f}±{result['std_latency']:.3f}s\")\n",
    "        print(f\"  Throughput: {result['throughput']:.1f} tokens/sec\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "\n",
    "print(\"PyTorch benchmarking complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec2d5e",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e6e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "if pytorch_results:\n",
    "    models = list(pytorch_results.keys())\n",
    "    pytorch_latencies = [pytorch_results[m]['mean_latency'] for m in models]\n",
    "    pytorch_throughputs = [pytorch_results[m]['throughput'] for m in models]\n",
    "    \n",
    "    # Note: TensorRT results would be added here after successful engine execution\n",
    "    # For now, showing PyTorch baseline results\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Latency comparison\n",
    "    x_pos = np.arange(len(models))\n",
    "    ax1.bar(x_pos, pytorch_latencies, alpha=0.7, label='PyTorch')\n",
    "    # ax1.bar(x_pos + 0.4, trt_latencies, alpha=0.7, label='TensorRT')  # Add when available\n",
    "    ax1.set_xlabel('Models')\n",
    "    ax1.set_ylabel('Latency (seconds)')\n",
    "    ax1.set_title('Inference Latency Comparison')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Throughput comparison\n",
    "    ax2.bar(x_pos, pytorch_throughputs, alpha=0.7, label='PyTorch')\n",
    "    # ax2.bar(x_pos + 0.4, trt_throughputs, alpha=0.7, label='TensorRT')  # Add when available\n",
    "    ax2.set_xlabel('Models')\n",
    "    ax2.set_ylabel('Throughput (tokens/sec)')\n",
    "    ax2.set_title('Inference Throughput Comparison')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(models)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"OPTIMIZATION RESULTS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for model in models:\n",
    "        pytorch_result = pytorch_results[model]\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(f\"  PyTorch Latency: {pytorch_result['mean_latency']:.3f}s\")\n",
    "        print(f\"  PyTorch Throughput: {pytorch_result['throughput']:.1f} tokens/sec\")\n",
    "        # Add TensorRT results when available\n",
    "        # speedup = pytorch_latency / trt_latency\n",
    "        # print(f\"  Speedup: {speedup:.2f}x\")\n",
    "\n",
    "else:\n",
    "    print(\"No benchmark results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77735687",
   "metadata": {},
   "source": [
    "## Save Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c484169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "optimization_results = {\n",
    "    'timestamp': timestamp,\n",
    "    'optimization_type': 'tensorrt',\n",
    "    'pytorch_results': pytorch_results,\n",
    "    'tensorrt_engines': list(trt_engines.keys()),\n",
    "    'precision': 'fp16',\n",
    "    'notes': 'Initial TensorRT optimization setup and PyTorch baseline'\n",
    "}\n",
    "\n",
    "filename = f\"tensorrt_optimization_{timestamp}.json\"\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(optimization_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to {filename}\")\n",
    "print(\"\\nTensorRT optimization setup complete!\")\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Fine-tune TensorRT engine parameters\")\n",
    "print(\"2. Add INT8 quantization\")\n",
    "print(\"3. Optimize for specific hardware\")\n",
    "print(\"4. Implement custom CUDA kernels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781fb07",
   "metadata": {},
   "source": [
    "## TensorRT Optimization Summary\n",
    "\n",
    "**Setup Complete**:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
